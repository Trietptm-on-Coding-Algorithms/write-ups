#Self-modifying code

Informally speaking, a self-modifying program is a program whose *some executed instructions are generated by the program itself*. However, self-modifying programs are not really different from "normal" programs, they are just much less popular. 

## Introduction

Though code and data are distinguished in both Von Neumann and Harvard architecture (Harvard architecture even does not allow modifying the memory zone for instructions), we may notice that any instruction must be fetched, decoded and executed by the CPU, so under view point of the CPU, [code is rather data](http://c2.com/cgi/wiki?DataAndCodeAreTheSameThing): *an instruction cannot not run itself, it must be interpreted by the CPU*. So no execution models can prohibit writing "self-modifying" programs.

Beside low-level reasons about performance, one of reason for which self-modifying code is not popular, is that self-modifying programs are very hard to be reasoned (i.e. understood, verified, etc.). None of high-level programming languages supports directly self-modifying code (though we can always write self-modifying programs in any Turing-complete language, of course). Formal semantics of programming languages, proof about programs, program analysis methods, most are constructed based on models without self-modifying code.

It might worth noting that self-modifying code is absolutely no more powerful: *any program using self-modifying code can be transformed into an equivalent program without self-modifying code*. The idea here [5] is to consider code of the original program as data, then design a non-self-modifying interpreter that interprets the (original) program.

### Some kinds of self-modifying code

One of the most popular kind of self-modifying programs is known under the name "packing" (e.g. [UPX](http://upx.sourceforge.net/)): the original instructions are unpacked first into the memory, then the control is divert to the unpacked instructions. This technique is very popular in malwares [4], which packed itself to avoid signature-based detection or to make it difficult to analysis.

Another kind consists of [JIT](https://en.wikipedia.org/wiki/Just-in-time_compilation) where the instructions of programs can be continuously generated in execution. This technique is normally used in high-performance interpreters (e.g. Python, JVM/.Net based languages).

One should distinguish these approaches, in the packed code, once the program is unpacked completely, it does not change itself again, there is a "transition point": before that the instructions are generated, after that they become stable. For JITed code, there is no such transition point.

### Writing self-modifying code

Once we recognize that "code" is nothing but "data", writing self-modifying code is not different from writing normal code. The execution of a program using self-modifying code can be separated into stages: there are instructions generating other instructions, and these instructions can even generated by other instructions. The principles are presented, for example in [1] for untyped or in [2] for typed languages, a theoretical base for typed staged computation is presented in [3]. A more practical discussion to write [nontrivial self-modifying programs](http://valerieaurora.org/synthesis/SynthesisOS/ch3.html) is presented in the chapter 3 of [6].

**Remark:** *in the following parts, we introduce new techniques in writing self-modifying code. Strictly speaking, these techniques are presented already in theoretical research papers; but they are not yet applied in practice so they look quite different from packing and JIT.*

In the examples, from `klein32.asm` to `klein32_staged3.asm` in `lectures/10self_modifying_code/src`, we have implemented several versions of [insertion sort](https://en.wikipedia.org/wiki/Insertion_sort) using self-modifying code. Before regarding the source codes with comments, we suggest students testing the binary `klein32`, for example:

    ./klein32 6 3 7 9 1
    
and examining the `insertion` function which is called at `0x8048548` to get some intuition about a self-modifying program.

  - `klein32.asm`: this is a direct implementation of insertion sort, no self-modifying code.
  - `klein32_staged0.asm`: in `klein32.asm`, the instruction 
  
        forLoop:
          cmp edx, [ebp+0x8] 
  
    at `forLoop` is used to compare the index `i` (stored in `edx`) with the length of the input array (stored in `[ebp + 0x8]`), but whenever the array's length is known, we can replace `[ebp+0x8]` by this value, so the instruction canB be regenerated as a comparison between `edx` and an immediate value
  
        forLoop:
          cmd edx, length_of_array
  
    The following instructions (see `klein32_staged0.asm` for more detail) will do that:
  
        insertion:
          ...
          mov ecx, [esp+0x4]
          cmp ecx, 0x7f                                         ; check the immediate value
          ja longCompare

          mov word [eax+(longCompare - insertion)], 0xfa83      ; opcode of the short comparison 
          mov byte [eax+(longCompare - insertion + 0x2)], cl    ; immediate value

          jmp storeRegs

        longCompare:
          mov word [eax+(longCompare - insertion)], 0xfa81      ; opcode for the long comparison
          mov dword [eax+(longCompare - insertion + 0x2)], ecx  ; immediate value
          
          ...
        
        forLoop:
          db 0xff, 0x20, 0x90, 0x90, 0x87, 0xcb ; these instructions will be regenerated
          
    There are two kind of opcodes for a comparison between `edx` and an immediate value: one is `0xfa83` used when the immediate value is not greater than `0x7f`, another is `0xfa81` (c.f. `longCompare`), that is the reason why we have used the comparison `cmd ecx, 0x7f`
    
    Q: There are better implementations to generate the instruction at `forLoop`, can you give one?
    
    Self-modifying code has a natural effect for *obfuscation* (here, this is anti-static analysis), we may note that the buffer at `forLoop` is disassembled as:
    
        forLoop:
          jmp [eax]
          nop
          nop
          xchg ebx, ebx
          
    which have nothing in common with the real instruction generated in runtime. The regenerated code would be *more efficient* since the length of the array is hard-coded directly in the instruction, CPU does not need request value of [ebp + 0x8] from the memory.
    
  - `klein32_staged1.asm`: in `klein32.asm`, if the index `i` reaches the length of the input array, then the sorting algorithm stops by:
  
        forLoop:
          cmp edx, [ebp+0x8]
          je stop
          
          ...
        
        stop:
          ...
          ret
          
          
    We obtain the same effect as `je stop` by a runtime generated direct jump, the idea is:
    
        if (i == arrayLength)
          jump stop 
        else
          jump $+2
          
    which is implemented in `klein32_staged1.asm` as:
    
        forLoop:
          ...
          mov bx, [eax]
          mov bh, 0x24
          cmovne bx, [eax]
          mov [eax], bx
          db 0x74, 0x00       ; will be regenerated when the index is equal to the length of the array
          
  - `klein32_staged2.asm`: once the instructions before `forLoop` have been executed, they are not executed again, then we can safely remove them or write some garbage data on them, that creates an effect of obfuscation, an example is in `klein32_staged2.asm`:
  
        forLoop:
          ...
          mov [eax-0x40], edx ; destroy executed instruction by garbage data
          
  - `klein32_staged3.asm`: gives the implementation with detail comments of the binary `knot32`.
    
## Homework

### Binaries and source codes

  * The binaries ("knot32" and "ecureuil") in the folder "bin" are supposed to use as homework, their source codes with detailed comments are in the folder "src". 

  * All binaries can be created (and have been created) using the Makefile, a small notice is that we have used [sstrip](https://github.com/BR903/ELFkickers) to strip binaries. This tool may not available yet on every Linux distribution, but we can always compile it from source code.

        make knot => generate knot and knot_simple binaries (64 bit version)
        make knot32 => for 32 bit version
        make ecureuil => generate ecureuil binary
        make all => generate all 

  * However, the compiled binaries may have some differences under different version of (assembly and C/C++) compilers . For example, "ecureuil" requires some manually computed constants on code alignment; for "knot", the technique of writing self-modifying code has some restrictions in x86, mostly because of lacking of the relative addressing mode (there is no such restriction in x86-64), and some instructions are encrypted using the runtime address of program as decryption key. So we strongly RECOMMEND using the existing binaries in the folder "bin" (recompile using Makefile is always possible, but some constants need to be changed).
     
  * The binary "ecureuil" is supposed to be more difficult than "knot", but it really depends on the skill and the used tools. But `knot` is better in showing self-modifying code techniques.

### Questions

  The questions can be used as hint for the challenges, they lead eventually to the final answers. There are obviously other approaches to solve them.

#### knot
  *Note:* this is a `64` bit challenge (not used as homework).
  
  1. What is the entry-point address of the binary?  
  A. `0x400470`

  2. How many command line arguments does the program require?  
  A. Exactly 1

  3. What is the address of the function (let's call it `knot`) consuming the flag?  
  A. `0x60102c`

  4. Does `knot` locate on the same segment with `main` (which locates at 0x400430)?  
  A. No, they are on different segments.

  5. Is the segment of `main` is writable? How about the segment of "knot"?  
  A: The segment containing "main" is not writable, but one containing "knot" is writable. Both of them are executable, of course.

  6. What is the purpose of the first two instructions of `knot`? Are they persistent (i.e. they are not modified in running time)?  
  A. The first two instructions generate two other instructions at 0x601040. They are  not persistent, they will be modified later.

  7. `knot` returns to where it is called (i.e. `0x400450`), how can it do that?  
  A. There is a `ret` instruction at `0x6010ac` which will be generated when needed, normally we do not see it.

  8. There is a loop in this function, how does it work?  
  A. There is a loop starts from `0x601066` to `0x601131`, the control flow is diverted by  a `ret` instruction at the end, the starting address is pushed into the stack before.

  9. What is the algorithm of `knot`?  
  A. A clear implementation of the same algorithm is in "knot.cpp"

  10. What is the good flag? :-)  
  A. There might exist several good flags, one of them is "H3Nr1P01NC4r3"

#### knot32

  1. Which is the address of the function (we call it `knot`) consuming the input flag?  
  A. `0x804a117`

  2. How the return value of `knot` is used?  
  A. The return value is compared with `0xa2edc5c4`, if equal then the program prints "Bravo, you got the key!!!", otherwise it prints "Try again, please :-)"

  3. Is the segment containing `knot` is writable? How about the segment containing "main" (which can be found at `0x804a117`)?
  A. The segment containing the function is writable, but the segment containing "main" is not. Note that the program has no sections.

  4. What is the purpose of the first 5 instructions of `knot`? Is there another method for this?  
    A. The first 5 instructions present a unpopular trick to get *runtime address* of `knot`. Since `knot` is activated by a `call`, they parse this instruction to get the relative offset (with respect to where it is called) of the called function, then get the return address from the stack, the runtime address of `knot` is equal to the sum of offset and the return address.  
    There is a more popular technique to get the runtime address, that is to call a simple piece of code:

        getIP:
          mov eax, [esp]
          ret

  5. Let's disassemble statically `knot`, what do you think about `pushf` at `0x804a130`, is it executed? What about the next instruction at `0x804a130`?  
     A. The instruction at `0x804a130` is not real, it is indeed modified in running time. The next instruction is modified too (note that most instructions of "knot" are modified in runtime).

  6. What is the purpose of 4 instructions from `0x804a149` to `0x804a153`?  
  A. They replace all instructions starting from the beginning of "knot" by a `NOP` sled.

  7. Can you recognize a loop in "knot"? (i.e. where is the address of the jump back instruction?, what is the technique used to hide this jump?)  
  A. There is a jump back `jmp ecx` at `0x804a19e`. This instruction is hidden, beside self-modifying code, by the [instruction overlapping](http://reverseengineering.stackexchange.com/questions/1531/what-is-overlapping-instructions-obfuscation) technique: there is a "jump to itself" instruction just before this "jmp ecx" (a basic disassembler, or even IDAPro cannot recognize this hidden jump).

  8. Is the jump back address is fixed? How it is calculated?  
  A. The jump back address is not fixed, it is instead calculated from the input.
  
  9. There are some instructions generated just before jumping back, what are they?  
  A. They are `add edx, eax` (generated by the instruction at `0x804a194`) and `rol edx, 0x8` (generated by the instruction at `0x804a19a`).

  9. There seems to exist only a single loop in "knot", but how the function can return back?  
  A. There is another `jmp` but it is generated only in runtime at the address `0x804a173`, moreover the correct target jump address is generated only when `knot` parse the last character (i.e. zero) of the input.
  
  10. How the input of "knot" is used? Can you describe how the return value of `knot` is calculated?  
  A. The algorithm is described clearly in `src/Program.cs`

  11. Until here, you should know the good password :-), what is it?  
  A. There may exist many, one of them is `H3Nr1P01NC4r3`.
  
  12. Can `knot` be "unpacked" in classical sense (i.e. detect OEP, dump memory, etc)? (bonus question :-))  
  A. Normally, no. The binary does not have a static form, it continuously modifies itself.

#### ecureuil
  1. "ecureuil" creates new processes in execution, how does it do?  
  A. It calls the system call `fork` at `0x8048573`

  2. There is a spin dead-lock (i.e. a jump to itself instruction) in the parent process, what is it address?  
  A. `0x8048588`

  3. Does the spin make the parent wait forever?
  A. No, this dead-lock will be unlocked by the child process. Concretely, the `EIP` of the parent process will be modified.

  4. There is also a spin dead-lock in the child process, what is it address?, how can it be unlocked?
  A. This is `0x80486c9`. The parent process will modify the "jump to itself" instruction to `jmp $+4`. 

  5. The input flag is manipulated by a loop. What is its address? Which process (i.e. parent or child) executes this loop?
  A. `0x80485a9`. The parent process execute this loop.

  6. How many iterations does the loop do?  
  A. This is quite tricky, statically we observe that there are 6 iterations, but there is indeed 7. The loop bound number is modified in running time by the child  process. Concretely, the instruction `cmp ecx, 6` at 0x80485cc will be modified to `cmp ecx, 7` in running time.

  7. Do you see the string "Yes!" or "Nop." in the binary?  
  A. They exist, of course. But not under a "normal" form of string (i.e. a zero-terminated sequence of characters), they are simply encoded as 32 bits numbers. The string "Yes!" is `0x21736559` (c.f. instruction at `0x80486f8`), and "Nop." is `0x2e706f4e` (c.f. instruction at `0x80486EEO`).

  8. Which process (i.e. parent or child) prints these strings? How is its control flow modified? (bonus question)  
  A. The parent process. This process is blocked the second time at the spin dead-lock, and once again the child process unlocks this spin.

  9. How is the manipulated password is checked?
  A. It is compared with the value `0xb32d7eec`. This is proceeded in the child process.

  10. What is the flag checking algorithm?  
  A. Please see the source code in C++

  11. What is the good flag? :-)  
  A. There may exist several, one of them is `600)!)34`

## References

[1] Ulrik Jorring and William L. Scherlis. Compilers and Staging Transformations.  
[2] Walid Taha. Multi-Stage Programming: Its Theory and Applications.  
[3] Rowan Davies and Frank Pfenning. A Modal Analysis of Staged Computation.  
[4] Kevin A. Roundy and Barton P. Miller. Binary-Code Obfuscations in Prevalent Packer Tools.  
[5] Guillaume Bonfante, Jean-Yves Marion and Daniel Reynaud. A Computability Perspective on Self-modifying Programs.  
[6] Alexia Massalin. Synthesis: An Efficient Implementation of Fundamental Operating System Services.
